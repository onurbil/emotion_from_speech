{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_Emotion_from_Speech_with_recorded_audio.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6NYTcAvln4J"
      },
      "source": [
        "### Mount Google Drive\n",
        "\n",
        "**Requires dataset_tensor.npy file in \"/Colab Notebooks/Emotion from Speech/\" folder!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XNfw-0EfZ4B"
      },
      "source": [
        "! pip install -q pyyaml h5py  # Required to save models in HDF5 format\r\n",
        "! pip install torch\r\n",
        "! pip install tqdm\r\n",
        "! pip install torchsummary\r\n",
        "! pip install bayesian-optimization\r\n",
        "! pip install ffmpeg-python\r\n",
        "! pip install pydub\r\n",
        "\r\n",
        "import torch\r\n",
        "from torchsummary import summary\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaNZqkhqfjiJ"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "PATH = '/content/drive/My Drive/Colab Notebooks/Emotion from Speech/'\n",
        "DATA_PATH = PATH + 'Data/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHxUZWqils8I"
      },
      "source": [
        "# Clone github repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmKol2g8WU1d"
      },
      "source": [
        "git_username = ''\n",
        "git_token =  ''\n",
        "\n",
        "if git_username == '':\n",
        "  print('Github username:')\n",
        "  git_username = %sx read -p ''\n",
        "  git_username = git_username[0]\n",
        "\n",
        "if git_token == '':\n",
        "  print('Github access token (https://github.com/settings/tokens):')\n",
        "  print('Github Token:')\n",
        "  git_token = %sx read -p ''\n",
        "  git_token = git_token[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KA6Ej0xOjzbu"
      },
      "source": [
        "# Clone the entire repo.\n",
        "%cd /content\n",
        "!git clone -l -s https://$git_username:$git_token@github.com/onurbil/emotion_from_speech.git emotion_from_speech\n",
        "%cd emotion_from_speech\n",
        "!ls\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmfy2j6FkjLo"
      },
      "source": [
        "import sys\n",
        "\n",
        "REPO_PATH = '/content/emotion_from_speech'\n",
        "\n",
        "sys.path.append(REPO_PATH)\n",
        "print(sys.path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGHJYwjUJ7Ge"
      },
      "source": [
        "# Get data and save it to your Drive\r\n",
        "\r\n",
        "** Only if you don't have it saved in your drive or want to update it **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPsnGRZHKANI"
      },
      "source": [
        "from google.colab import files\r\n",
        "print(\"\\nUpload your kaggle.json (find it on your kaggle account under API)\\n\")\r\n",
        "files.upload()\r\n",
        "import shutil\r\n",
        "import os\r\n",
        "from emotion_from_speech import main\r\n",
        "from emotion_from_speech import dataset\r\n",
        "import big_dataset\r\n",
        "import data_augmentation\r\n",
        "data_augmentation.augment_dataset('/content/TESS Toronto emotional speech set data', os.path.join(REPO_PATH, 'augmentation_sounds'))\r\n",
        "\r\n",
        "!pip install -q kaggle\r\n",
        "!mkdir -p ~/.kaggle\r\n",
        "!cp kaggle.json ~/.kaggle/\r\n",
        "!rm *.zip\r\n",
        "!kaggle datasets download -d uldisvalainis/audio-emotions\r\n",
        "!unzip \\*.zip  && rm *.zip\r\n",
        "big_dataset.process_dataset()\r\n",
        "\r\n",
        "filesToMove = [\r\n",
        "               'data_list.npy', \r\n",
        "               'data_list_engine-0.3.npy',\r\n",
        "               'data_list_h_noise-0.3.npy',\r\n",
        "               'data_list_l_noise-0.3.npy',\r\n",
        "               'data_list_piano-0.4.npy',\r\n",
        "               'data_list_talking-0.4.npy',\r\n",
        "               'big_data_list.npy',\r\n",
        "               ]\r\n",
        "\r\n",
        "os.makedirs(os.path.dirname(DATA_PATH), exist_ok=True)\r\n",
        "for files in filesToMove:\r\n",
        "  shutil.copy(files, DATA_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NTfnysd6TDj"
      },
      "source": [
        "\r\n",
        "---\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "# Experiments\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCbok4hOM1tP"
      },
      "source": [
        "## Clean dataset training\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRyMOq1gUepb"
      },
      "source": [
        "import torch \r\n",
        "\r\n",
        "if torch.cuda.is_available():\r\n",
        "    device = torch.cuda.current_device()\r\n",
        "    print('Current device:', torch.cuda.get_device_name(device))\r\n",
        "else:\r\n",
        "    print('Failed to find GPU. Will use CPU.')\r\n",
        "    device = 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndKDDsX2QVwC"
      },
      "source": [
        "from training import load_dataset\r\n",
        "from training import train_model\r\n",
        "from training import test_hyper_parameters\r\n",
        "from models import LSTM, GRU, VanillaRNN\r\n",
        "import os\r\n",
        "dataset_path = os.path.join(DATA_PATH,'big_data_list.npy')\r\n",
        "\r\n",
        "## Parameters:\r\n",
        "num_runs = 1\r\n",
        "patience = 20\r\n",
        "batch_size = 64\r\n",
        "num_epochs = 300\r\n",
        "learning_rate = 0.001\r\n",
        "weight_decay = 0.01\r\n",
        "model_cls = GRU\r\n",
        "model_args = {\r\n",
        "    'num_layers': 6,\r\n",
        "    'hidden_size': 256,\r\n",
        "    'linear_size': 128,\r\n",
        "    'bidirectional': True,\r\n",
        "}\r\n",
        "\r\n",
        "results = test_hyper_parameters(num_runs=num_runs, dataset_path=dataset_path,\r\n",
        "                                batch_size=batch_size,\r\n",
        "                                num_epochs=num_epochs,\r\n",
        "                                learning_rate=learning_rate,\r\n",
        "                                weight_decay=weight_decay,\r\n",
        "                                model_cls=model_cls, model_args=model_args,\r\n",
        "                                device=device, patience=patience, verbose=1)\r\n",
        "\r\n",
        "print(results[0])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6-d8qRRgK56"
      },
      "source": [
        "## Augmented dataset training\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSciWSNRgK58"
      },
      "source": [
        "import torch \r\n",
        "\r\n",
        "if torch.cuda.is_available():\r\n",
        "    device = torch.cuda.current_device()\r\n",
        "    print('Current device:', torch.cuda.get_device_name(device))\r\n",
        "else:\r\n",
        "    print('Failed to find GPU. Will use CPU.')\r\n",
        "    device = 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOnXYG-OgK59"
      },
      "source": [
        "from training import load_dataset\r\n",
        "from training import train_model\r\n",
        "from training import test_hyper_parameters_augmented\r\n",
        "from models import LSTM, GRU, VanillaRNN\r\n",
        "import os\r\n",
        "\r\n",
        "dataset_path = os.path.join(DATA_PATH, 'data_list.npy')\r\n",
        "train_dataset_paths = [\r\n",
        "    dataset_path,\r\n",
        "    os.path.join(DATA_PATH, 'data_list_engine-0.3.npy'),\r\n",
        "    os.path.join(DATA_PATH, 'data_list_piano-0.4.npy'),\r\n",
        "    os.path.join(DATA_PATH, 'data_list_l_noise-0.3.npy'),\r\n",
        "]\r\n",
        "test_dataset_path = os.path.join(DATA_PATH, 'data_list_talking-0.4.npy')\r\n",
        "\r\n",
        "## Parameters:\r\n",
        "num_runs = 3\r\n",
        "patience = 20\r\n",
        "batch_size = 64\r\n",
        "num_epochs = 300\r\n",
        "learning_rate = 0.001\r\n",
        "weight_decay = 0.01\r\n",
        "model_cls = GRU\r\n",
        "model_args = {\r\n",
        "    'num_layers': 2,\r\n",
        "    'hidden_size': 256,\r\n",
        "    'linear_size': 128,\r\n",
        "    'bidirectional': True,\r\n",
        "}\r\n",
        "\r\n",
        "results = test_hyper_parameters_augmented(num_runs=num_runs,\r\n",
        "                                          clean_dataset_path=dataset_path,\r\n",
        "                                          train_dataset_paths=train_dataset_paths,\r\n",
        "                                          test_dataset_path=test_dataset_path,\r\n",
        "                                          batch_size=batch_size,\r\n",
        "                                          num_epochs=num_epochs,\r\n",
        "                                          learning_rate=learning_rate,\r\n",
        "                                          weight_decay=weight_decay,\r\n",
        "                                          model_cls=VanillaRNN,\r\n",
        "                                          model_args=model_args,\r\n",
        "                                          device=device, patience=patience,\r\n",
        "                                          verbose=1)\r\n",
        "\r\n",
        "# print(results[0])\r\n",
        "# print(results[3])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xllwucHgrR7w"
      },
      "source": [
        "# Tests with recorded audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89qD6H5Vc0ot"
      },
      "source": [
        "## Record Audio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sA2WhjHalSR"
      },
      "source": [
        "from record_audio import save_audio\r\n",
        "audio_name = 'recording'\r\n",
        "save_audio(DATA_PATH, name = audio_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZfDzOzBc3Yf"
      },
      "source": [
        "## Test the recorded audio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ht16Kr1ra9_"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "dataset_path = os.path.join(DATA_PATH,audio_name + '.npy')\r\n",
        "dataset_path = os.path.join(DATA_PATH,'data_list.npy')\r\n",
        "\r\n",
        "data_list = np.load(dataset_path, allow_pickle=True)\r\n",
        "train_loader, _, _, _, _ = load_dataset(dataset_path, 1, shuffle_dataset=False)\r\n",
        "\r\n",
        "model = results[-1].to(device)\r\n",
        "model.eval()\r\n",
        "\r\n",
        "dataiter = iter(train_loader)\r\n",
        "record, labels = dataiter.next()\r\n",
        "# print(record)\r\n",
        "record = record.to(device)\r\n",
        "y_valid_pred = model(record)\r\n",
        "\r\n",
        "print(y_valid_pred.data)\r\n",
        "print('\\n')\r\n",
        "print(torch.exp(y_valid_pred.data))\r\n",
        "print('\\n')\r\n",
        "_, y_valid_pred = torch.max(y_valid_pred.data, 1)\r\n",
        "\r\n",
        "names=[\"angry\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"ps\", \"sad\"]\r\n",
        "\r\n",
        "print(y_valid_pred)\r\n",
        "print(\"\\n\")\r\n",
        "print(data_list[0][0].shape)\r\n",
        "print(data_list[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}